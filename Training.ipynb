{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f5e7a43",
   "metadata": {},
   "source": [
    "# Anomaly Detection Training\n",
    "\n",
    "This notebook implements the training and evaluation of the anomaly detection model using the CustomVGG architecture, matching the latest codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ead3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "import gc\n",
    "\n",
    "from utils.dataloader import get_train_loaders, get_test_loaders\n",
    "from utils.model import CustomVGG\n",
    "from utils.helper import train, evaluate, plot_dataset_comparison\n",
    "from utils.constants import NEG_CLASS, INPUT_IMG_SIZE\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d677628",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc94d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "activation = nn.ReLU  # or nn.LeakyReLU\n",
    "num_neurons = 64  # or 128\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 32  # or 16\n",
    "LEARNING_RATE = 0.0005  # or 0.0001\n",
    "NUM_EPOCHS = 20\n",
    "CLASS_WEIGHTS = torch.tensor([1.0, 3.0])\n",
    "CLASSIFICATION_THRESHOLD = 16.00\n",
    "\n",
    "# Optimizer parameters\n",
    "WEIGHT_DECAY = 1e-4\n",
    "SCHEDULER_FACTOR = 0.5\n",
    "SCHEDULER_PATIENCE = 5\n",
    "MIN_LR = 1e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02edf42e",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af79f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = get_train_loaders(BATCH_SIZE)\n",
    "test_loader = get_test_loaders(BATCH_SIZE)\n",
    "print(f'Train batches: {len(train_loader)}, Validation batches: {len(val_loader)}, Test batches: {len(test_loader)}')\n",
    "\n",
    "# Plot dataset comparison\n",
    "plot_dataset_comparison(train_loader, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e744090",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4402ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomVGG(activation=activation, num_neurons=num_neurons).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS.to(device), label_smoothing=0.1)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=SCHEDULER_FACTOR, \n",
    "    patience=SCHEDULER_PATIENCE, \n",
    "    min_lr=MIN_LR,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Show model summary\n",
    "model.show_summary(batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe08f3d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e59caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = train(\n",
    "    train_loader, val_loader, model, optimizer, criterion,\n",
    "    NUM_EPOCHS, device, target_train_accuracy=0.90, scheduler=scheduler\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f772a58",
   "metadata": {},
   "source": [
    "## Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdbb822",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss Curves')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_acc'], label='Train Acc')\n",
    "plt.plot(history['val_acc'], label='Val Acc')\n",
    "plt.title('Accuracy Curves')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde2456e",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b92d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, loss, conf_matrix = evaluate(model, test_loader, device, threshold=CLASSIFICATION_THRESHOLD)\n",
    "print(f'Accuracy: {accuracy:.4f}, Loss: {loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7e0102",
   "metadata": {},
   "source": [
    "## Additional Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e779f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        if isinstance(outputs, tuple):\n",
    "            outputs = outputs[0]\n",
    "        preds = (outputs[:, 1] > CLASSIFICATION_THRESHOLD).long()\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(f'Balanced Accuracy: {balanced_acc:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "# Print class distribution\n",
    "print(\"\n",
    "Class distribution in predictions:\")\n",
    "print(np.bincount(y_pred))\n",
    "print(\"\n",
    "Class distribution in true labels:\")\n",
    "print(np.bincount(y_true))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c932da",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21fdabea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'activation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create experiment name\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m experiment_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mactivation\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_neurons\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLEARNING_RATE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBATCH_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Save model and training history\u001b[39;00m\n\u001b[1;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'activation' is not defined"
     ]
    }
   ],
   "source": [
    "# Create experiment name\n",
    "experiment_name = f\"{activation.__name__}_{num_neurons}_{LEARNING_RATE}_{optimizer.__class__.__name__}_{BATCH_SIZE}\"\n",
    "\n",
    "# Save model and training history\n",
    "os.makedirs('weights', exist_ok=True)\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'activation': activation,\n",
    "    'num_neurons': num_neurons,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'optimizer': optimizer.__class__.__name__,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'history': history\n",
    "}, f\"weights/model_{experiment_name}.pt\")\n",
    "\n",
    "print(f'Model saved to weights/model_{experiment_name}.pt')\n",
    "\n",
    "# Save metrics to CSV\n",
    "os.makedirs('results/metrics', exist_ok=True)\n",
    "metrics_df = pd.DataFrame([{\n",
    "    'activation': activation.__name__,\n",
    "    'num_neurons': num_neurons,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'optimizer': optimizer.__class__.__name__,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'accuracy': accuracy,\n",
    "    'loss': loss,\n",
    "    'balanced_accuracy': balanced_acc,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1_score': f1\n",
    "}])\n",
    "\n",
    "# Append to existing results or create new file\n",
    "if os.path.exists('results/metrics/experiment_results.csv'):\n",
    "    existing_df = pd.read_csv('results/metrics/experiment_results.csv')\n",
    "    metrics_df = pd.concat([existing_df, metrics_df], ignore_index=True)\n",
    "metrics_df.to_csv('results/metrics/experiment_results.csv', index=False)\n",
    "print('Metrics saved to results/metrics/experiment_results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
